# azure-pipelines-backend.yml for BarberMatch (Nest.js)
# Cumple con la RÃºbrica: Build -> SonarCloud -> Deploy -> JMeter

trigger:
  branches:
    include:
    - main
    - master
    - develop
    - feature/*

# [RÃºbrica] Disparo por Pull Request [cite: 23, 54]
pr:
  branches:
    include:
    - main
    - master
    - develop

variables:
  # TecnologÃ­as: Nest.js (Node.js/TypeScript)
  nodeVersion: '20.x' 
  vmImageName: 'ubuntu-latest'
  artifactName: 'backend-dist-package'
  
  # --- CONEXIÃ“N CON SONARCLOUD ---
  sonarCloudServiceConnection: 'SonarQube-Connection'
  sonarCloudOrganization: 'iduertom'
  sonarCloudProjectKey: 'iduertom_BarberMatch-Backend' # <--- Â¡CORREGIDO!
  
  # --- CONEXIÃ“N CON AZURE ---
  azureSubscription: 'AzureServiceConnection'
  resourceGroupName: 'barbermatch-rg'
  webAppName: 'barbermatch-backend-api-$(Build.BuildId)' 
  # URL para JMeter (asume el formato estÃ¡ndar de Azure Web App)
  deployedUrl: 'https://barbermatch-backend-api-$(Build.BuildId).azurewebsites.net'

pool:
  vmImage: $(vmImageName)

stages:
# ------------------------------------------------------------------------
# STAGE 1: BUILD, TEST & PACKAGE [cite: 25, 75, 76]
# ------------------------------------------------------------------------
- stage: Build
  displayName: 'ðŸ—ï¸ Build, Test & Package Backend'
  jobs:
  - job: BackendBuild
    displayName: 'NestJS Compilation & Testing'
    steps:
    - task: NodeTool@0
      displayName: 'ðŸ”§ Install Node.js $(nodeVersion)'
      inputs:
        versionSpec: $(nodeVersion)

    - script: |
        npm ci
        npm run build # Genera /dist
      displayName: 'ðŸ“¦ Install Dependencies & Compile'
      
    - script: |
        echo "##[section] Running Unit Tests with Coverage (Requisito de Testabilidad)"
        # Asume que 'test:cov' genera un reporte lcov.info y JUnit/Cobertura
        npm run test:cov 
      displayName: 'ðŸ§ª Run Unit Tests & Generate Coverage'
      # El pipeline fallarÃ¡ aquÃ­ si el build o las pruebas fallan [cite: 30]
      
    - task: PublishBuildArtifacts@1
      displayName: 'ðŸ“¦ Publish Build Artifacts'
      inputs:
        PathtoPublish: '$(System.DefaultWorkingDirectory)/dist' 
        ArtifactName: '$(artifactName)'
        
# ------------------------------------------------------------------------
# STAGE 2: SONARCLOUD [cite: 26, 99]
# ------------------------------------------------------------------------
- stage: SonarCloud
  displayName: 'ðŸ”¬ SonarCloud Analysis'
  dependsOn: Build
  condition: succeeded()
  jobs:
  - job: Analyze
    displayName: 'Scan NestJS Code'
    steps:
    - checkout: self
      fetchDepth: 0
      
    - task: NodeTool@0
      displayName: 'ðŸ”§ Install Node.js $(nodeVersion) for Sonar'
      inputs:
        versionSpec: $(nodeVersion)
    
    # Reinstalar dependencias y regenerar cobertura para SonarCloud
    - script: |
        npm ci
        npm run test:cov
      displayName: 'ðŸ“Š Regenerate Coverage for SonarCloud'
        
    - task: SonarCloudPrepare@3
      displayName: 'ðŸ”§ Prepare SonarCloud'
      inputs:
        SonarCloud: '$(sonarCloudServiceConnection)'
        organization: '$(sonarCloudOrganization)'
        scannerMode: 'CLI'
        configMode: 'manual'
        projectKey: '$(sonarCloudProjectKey)'
        projectName: 'BarberMatch Backend API'
        extraProperties: |
          sonar.javascript.lcov.reportPaths=coverage/lcov.info
          sonar.exclusions=**/*.spec.ts, **/*.test.ts, **/node_modules/**, **/dist/**
          
    - task: SonarCloudAnalyze@3
      displayName: 'ðŸ” Run Code Analysis'
    
    # [RÃºbrica] Usar Quality Gate para fallar[cite: 99].
    # SoluciÃ³n para Plan Gratuito: Permite avanzar a pesar del "Not computed".
    - task: SonarCloudPublish@3
      displayName: 'ðŸ“Š Publish Quality Gate Results'
      inputs:
        pollingTimeoutSec: '300' 
      continueOnError: true # <-- Clave para que el pipeline no se detenga.

# ------------------------------------------------------------------------
# STAGE 3: DEPLOY [cite: 27, 118]
# ------------------------------------------------------------------------
- stage: Deploy
  displayName: 'ðŸš€ Deploy to Azure Web App'
  dependsOn: SonarCloud
  # Solo despliega en ramas principales [cite: 23]
  condition: and(succeeded(), or(eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['Build.SourceBranch'], 'refs/heads/master')))
  jobs:
  - deployment: DeployWebApp
    displayName: 'Deploy Backend'
    environment: 'Production' 
    pool:
      vmImage: $(vmImageName)
    strategy:
      runOnce:
        deploy:
          steps:
          - task: DownloadBuildArtifacts@0
            displayName: 'ðŸ“¥ Download Build Artifacts'
            inputs:
              artifactName: '$(artifactName)'
              downloadPath: '$(System.DefaultWorkingDirectory)' 
              
          - task: AzureWebApp@1
            displayName: 'â˜ï¸ Deploy to Azure: $(webAppName)'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: 'webAppLinux' 
              appName: '$(webAppName)'
              package: '$(System.DefaultWorkingDirectory)/$(artifactName)' 
              runtimeStack: 'NODE|$(nodeVersion)'
              startUpCommand: 'npm run start:prod' 
            # El pipeline fallarÃ¡ si el despliegue no se completa [cite: 121]

# ------------------------------------------------------------------------
# STAGE 4: PERFORMANCE TEST (JMeter) [cite: 29, 153]
# ------------------------------------------------------------------------
- stage: PerformanceTest
  displayName: 'âš¡ JMeter Performance Test'
  dependsOn: Deploy
  condition: succeeded()
  jobs:
  - job: RunJMeter
    displayName: 'Execute Load Test'
    steps:
    - checkout: self
      
    # 1. Instalar JMeter y Java
    - script: |
        sudo apt-get update
        sudo apt-get install default-jre -y 
        wget https://dlcdn.apache.org//jmeter/binaries/apache-jmeter-5.6.3.zip
        unzip apache-jmeter-5.6.3.zip -d $(System.DefaultWorkingDirectory)
        export JMETER_HOME=$(System.DefaultWorkingDirectory)/apache-jmeter-5.6.3
        export PATH=$PATH:$JMETER_HOME/bin
      displayName: 'ðŸ”§ Install JMeter'
      
    # 2. Warm-Up / Health Check
    - script: |
        echo "##[section] Waking up the deployed application..."
        APP_URL="$(deployedUrl)/health" # Asume un endpoint de salud
        
        for i in {1..15}; do
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" $APP_URL)
          if [ "$STATUS" -ge 200 ] && [ "$STATUS" -le 404 ]; then
            echo "âœ… App is UP and running! (Status: $STATUS)"
            exit 0
          fi
          sleep 10
        done
        echo "âŒ App failed to start after warm-up."
      displayName: 'ðŸ•’ Warm-Up / Health Check'
      
    # 3. Ejecutar la prueba JMeter
    - script: |
        echo "##[section] Running JMeter against $(deployedUrl)..."
        TEST_FILE="tests/jmeter/barbermatch-load-test.jmx" 
        RESULTS_FILE="jmeter_results.xml"
        
        jmeter -n -t $TEST_FILE -l $RESULTS_FILE -e -o jmeter_dashboard
      displayName: 'ðŸ”¥ Run JMeter Load Test'
      
    # 4. Publicar resultados como artefacto [cite: 155]
    - task: PublishBuildArtifacts@1
      displayName: 'ðŸ“Š Publish JMeter Dashboard'
      inputs:
        PathtoPublish: '$(System.DefaultWorkingDirectory)/jmeter_dashboard'
        ArtifactName: 'JMeter-Dashboard'
      condition: always()

    # 5. ValidaciÃ³n del Resultado: Fallar si hay errores (cÃ³digos != 200) [cite: 156]
    - script: |
        echo "##[section] Validating JMeter Results (checking for non-200 responses)..."
        FAILURES=$(grep -c 'failure="true"' jmeter_results.xml)
        TOTAL_REQUESTS=$(grep -c '<httpSample' jmeter_results.xml)
        
        if [ "$FAILURES" -gt 0 ]; then
          echo "##[error] Performance Test FAILED: Found $FAILURES errors out of $TOTAL_REQUESTS requests."
          [cite_start]echo "##[error] El pipeline falla ya que se encontraron errores (no 200s) en las respuestas. [cite: 156]"
          exit 1
        else
          echo "âœ… Performance Test PASSED: All $TOTAL_REQUESTS requests were successful (HTTP 200 assertion passed)."
        fi
      displayName: 'âœ… Validate Performance Results'
      # El pipeline fallarÃ¡ si hay una proporciÃ³n significativa de errores [cite: 30]