# azure-pipelines-backend.yml for BarberMatch (Nest.js)
# Cumple con la R√∫brica: Build -> SonarCloud -> Deploy -> JMeter

trigger:
  branches:
    include:
    - main
    - master
    - develop
    - feature/*

# [R√∫brica] Disparo por Pull Request [cite: 23, 54]
pr:
  branches:
    include:
    - main
    - master
    - develop

variables:
  # Tecnolog√≠as: Nest.js (Node.js/TypeScript)
  nodeVersion: '20.x' 
  vmImageName: 'ubuntu-latest'
  artifactName: 'backend-dist-package'
  
  # --- CONEXI√ìN CON SONARCLOUD ---
  sonarCloudServiceConnection: 'SonarQube-Back'
  sonarCloudOrganization: 'iduertom'
  sonarCloudProjectKey: 'iduertom_backend' # <--- ¬°CORREGIDO!
  
  # --- CONEXI√ìN CON AZURE ---
  azureSubscription: 'AzureServiceConnection'
  resourceGroupName: 'barbermatch-rg'
  webAppName: 'iduertom_backend-api-$(Build.BuildId)'
  # URL para JMeter (asume el formato est√°ndar de Azure Web App)
  deployedUrl: 'https://iduertom_backend-api-$(Build.BuildId).azurewebsites.net'

pool:
  vmImage: $(vmImageName)

stages:
# ------------------------------------------------------------------------
# STAGE 1: BUILD, TEST & PACKAGE [cite: 25, 75, 76]
# ------------------------------------------------------------------------
- stage: Build
  displayName: 'üèóÔ∏è Build, Test & Package Backend'
  jobs:
  - job: BackendBuild
    displayName: 'NestJS Compilation & Testing'
    steps:
    - task: NodeTool@0
      displayName: 'üîß Install Node.js $(nodeVersion)'
      inputs:
        versionSpec: $(nodeVersion)

    - script: |
        npm ci
        npm run build # Genera /dist
      displayName: 'üì¶ Install Dependencies & Compile'
      
    - script: |
        echo "##[section] Running Unit Tests with Coverage (Requisito de Testabilidad)"
        # Asume que 'test:cov' genera un reporte lcov.info y JUnit/Cobertura
        npm run test:cov 
      displayName: 'üß™ Run Unit Tests & Generate Coverage'
      # El pipeline fallar√° aqu√≠ si el build o las pruebas fallan [cite: 30]
      
    - task: PublishBuildArtifacts@1
      displayName: 'üì¶ Publish Build Artifacts'
      inputs:
        PathtoPublish: '$(System.DefaultWorkingDirectory)/dist' 
        ArtifactName: '$(artifactName)'
        
# ------------------------------------------------------------------------
# STAGE 2: SONARCLOUD [cite: 26, 99]
# ------------------------------------------------------------------------
- stage: SonarCloud
  displayName: 'üî¨ SonarCloud Analysis'
  dependsOn: Build
  condition: succeeded()
  jobs:
  - job: Analyze
    displayName: 'Scan NestJS Code'
    steps:
    - checkout: self
      fetchDepth: 0
      
    - task: NodeTool@0
      displayName: 'üîß Install Node.js $(nodeVersion) for Sonar'
      inputs:
        versionSpec: $(nodeVersion)
    
    # Reinstalar dependencias y regenerar cobertura para SonarCloud
    - script: |
        npm ci
        npm run test:cov
      displayName: 'üìä Regenerate Coverage for SonarCloud'
        
    - task: SonarCloudPrepare@3
      displayName: 'üîß Prepare SonarCloud'
      inputs:
        SonarCloud: '$(sonarCloudServiceConnection)'
        organization: '$(sonarCloudOrganization)'
        scannerMode: 'CLI'
        configMode: 'file'
        
    # Create sonar-project.properties file with proper paths
    - script: |
        cat > sonar-project.properties << EOF
        sonar.projectKey=iduertom_backend
        sonar.projectName=BarberMatch Backend API
        sonar.sources=src
        sonar.language=ts
        sonar.sourceEncoding=UTF-8
        sonar.javascript.lcov.reportPaths=coverage/lcov.info
        sonar.exclusions=**/*.spec.ts,**/*.test.ts,**/node_modules/**,**/dist/**,**/coverage/**
        sonar.tests=src
        sonar.test.inclusions=**/*.spec.ts,**/*.test.ts
        sonar.coverage.exclusions=**/*.spec.ts,**/*.test.ts
        EOF
        
        echo "=== Verification ==="
        echo "Working directory: $(pwd)"
        ls -la src/ | head -10
        cat sonar-project.properties
      displayName: 'üìù Create sonar-project.properties with verification'
          
    - task: SonarCloudAnalyze@3
      displayName: 'üîç Run Code Analysis'
    
    # [R√∫brica] Usar Quality Gate para fallar[cite: 99].
    # Soluci√≥n para Plan Gratuito: Permite avanzar a pesar del "Not computed".
    - task: SonarCloudPublish@3
      displayName: 'üìä Publish Quality Gate Results'
      inputs:
        pollingTimeoutSec: '300' 
      continueOnError: true # <-- Clave para que el pipeline no se detenga.

# ------------------------------------------------------------------------
# STAGE 3: DEPLOY [cite: 27, 118]
# ------------------------------------------------------------------------
- stage: Deploy
  displayName: 'üöÄ Deploy to Azure Web App'
  dependsOn: SonarCloud
  # Solo despliega en ramas principales [cite: 23]
  condition: and(succeeded(), or(eq(variables['Build.SourceBranch'], 'refs/heads/main'), eq(variables['Build.SourceBranch'], 'refs/heads/master')))
  jobs:
  - deployment: DeployWebApp
    displayName: 'Deploy Backend'
    environment: 'Production' 
    pool:
      vmImage: $(vmImageName)
    strategy:
      runOnce:
        deploy:
          steps:
          - task: DownloadBuildArtifacts@0
            displayName: 'üì• Download Build Artifacts'
            inputs:
              artifactName: '$(artifactName)'
              downloadPath: '$(System.DefaultWorkingDirectory)' 
          
          # Create Azure Web App if it doesn't exist
          - task: AzureCLI@2
            displayName: 'üèóÔ∏è Create Azure Web App if not exists'
            inputs:
              azureSubscription: '$(azureSubscription)'
              scriptType: 'bash'
              scriptLocation: 'inlineScript'
              inlineScript: |
                # Check if resource group exists, create if not
                if ! az group show --name $(resourceGroupName) --output none 2>/dev/null; then
                  echo "Creating resource group $(resourceGroupName)..."
                  az group create --name $(resourceGroupName) --location "East US"
                fi
                
                # Check if app service plan exists, create if not
                PLAN_NAME="$(resourceGroupName)-plan"
                if ! az appservice plan show --name $PLAN_NAME --resource-group $(resourceGroupName) --output none 2>/dev/null; then
                  echo "Creating App Service Plan $PLAN_NAME..."
                  az appservice plan create --name $PLAN_NAME --resource-group $(resourceGroupName) --sku F1 --is-linux
                fi
                
                # Check if web app exists, create if not
                if ! az webapp show --name $(webAppName) --resource-group $(resourceGroupName) --output none 2>/dev/null; then
                  echo "Creating Web App $(webAppName)..."
                  az webapp create --name $(webAppName) --resource-group $(resourceGroupName) --plan $PLAN_NAME --runtime "NODE|$(nodeVersion)"
                  
                  # Configure startup command
                  az webapp config set --name $(webAppName) --resource-group $(resourceGroupName) --startup-file "npm run start:prod"
                else
                  echo "Web App $(webAppName) already exists"
                fi
              
          - task: AzureWebApp@1
            displayName: '‚òÅÔ∏è Deploy to Azure: $(webAppName)'
            inputs:
              azureSubscription: '$(azureSubscription)'
              appType: 'webAppLinux' 
              appName: '$(webAppName)'
              package: '$(System.DefaultWorkingDirectory)/$(artifactName)' 
              runtimeStack: 'NODE|$(nodeVersion)'
              startUpCommand: 'npm run start:prod'

# ------------------------------------------------------------------------
# STAGE 4: PERFORMANCE TEST (JMeter) [cite: 29, 153]
# ------------------------------------------------------------------------
- stage: PerformanceTest
  displayName: '‚ö° JMeter Performance Test'
  dependsOn: Deploy
  condition: succeeded()
  jobs:
  - job: RunJMeter
    displayName: 'Execute Load Test'
    steps:
    - checkout: self
      
    # 1. Instalar JMeter y Java
    - script: |
        sudo apt-get update
        sudo apt-get install default-jre -y 
        wget https://dlcdn.apache.org//jmeter/binaries/apache-jmeter-5.6.3.zip
        unzip apache-jmeter-5.6.3.zip -d $(System.DefaultWorkingDirectory)
        export JMETER_HOME=$(System.DefaultWorkingDirectory)/apache-jmeter-5.6.3
        export PATH=$PATH:$JMETER_HOME/bin
      displayName: 'üîß Install JMeter'
      
    # 2. Warm-Up / Health Check
    - script: |
        echo "##[section] Waking up the deployed application..."
        APP_URL="$(deployedUrl)/health" # Asume un endpoint de salud
        
        for i in {1..15}; do
          STATUS=$(curl -s -o /dev/null -w "%{http_code}" $APP_URL)
          if [ "$STATUS" -ge 200 ] && [ "$STATUS" -le 404 ]; then
            echo "‚úÖ App is UP and running! (Status: $STATUS)"
            exit 0
          fi
          sleep 10
        done
        echo "‚ùå App failed to start after warm-up."
      displayName: 'üïí Warm-Up / Health Check'
      
    # 3. Ejecutar la prueba JMeter
    - script: |
        echo "##[section] Running JMeter against $(deployedUrl)..."
        TEST_FILE="tests/jmeter/barbermatch-load-test.jmx" 
        RESULTS_FILE="jmeter_results.xml"
        
        jmeter -n -t $TEST_FILE -l $RESULTS_FILE -e -o jmeter_dashboard
      displayName: 'üî• Run JMeter Load Test'
      
    # 4. Publicar resultados como artefacto [cite: 155]
    - task: PublishBuildArtifacts@1
      displayName: 'üìä Publish JMeter Dashboard'
      inputs:
        PathtoPublish: '$(System.DefaultWorkingDirectory)/jmeter_dashboard'
        ArtifactName: 'JMeter-Dashboard'
      condition: always()

    # 5. Validaci√≥n del Resultado: Fallar si hay errores (c√≥digos != 200) [cite: 156]
    - script: |
        echo "##[section] Validating JMeter Results (checking for non-200 responses)..."
        FAILURES=$(grep -c 'failure="true"' jmeter_results.xml)
        TOTAL_REQUESTS=$(grep -c '<httpSample' jmeter_results.xml)
        
        if [ "$FAILURES" -gt 0 ]; then
          echo "##[error] Performance Test FAILED: Found $FAILURES errors out of $TOTAL_REQUESTS requests."
          [cite_start]echo "##[error] El pipeline falla ya que se encontraron errores (no 200s) en las respuestas. [cite: 156]"
          exit 1
        else
          echo "‚úÖ Performance Test PASSED: All $TOTAL_REQUESTS requests were successful (HTTP 200 assertion passed)."
        fi
      displayName: '‚úÖ Validate Performance Results'
      # El pipeline fallar√° si hay una proporci√≥n significativa de errores [cite: 30]